{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11ade76",
   "metadata": {},
   "source": [
    "## Twitter Text Data Gathering Report\n",
    "\n",
    "#### By Calvin Muller\n",
    "\n",
    "#### October 20, 2021\n",
    "\n",
    "In this report, I am using the Twitter API v2 to create a query to gather a series of tweets that may be helpful in answering a question I have about music release dates. The question I have is this: does the day that an album releases significantly affect how well it does, and how many people talk about it? Albums traditionally release at midnight of Friday, however there have been some well known artists that like to switch up the status quo and release albums at odd times (most notably of late Kanye releasing his highly anticipated \"Donda\" at 9am on a Sunday). Is this conducive to more talk about the album, more publicity? That's what I hope gathering this data might be able to help with.\n",
    "\n",
    "The query I developed will gather all recent tweets that are talking about an album, and what day that album is going to release. I've used a few common synonyms for \"releasing\" in hopes to get a wide variety of tweets. My hope is that these tweets will show which albums people are talking about more, as well as how many albums go the untraditional route of releasing on less-common days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0d7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e83d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = pd.read_csv('twitter_bearer_token.txt', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f53b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'Authorization' : 'Bearer {}'.format(bearer_token['Bearer_Token'].iloc[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f131eaf",
   "metadata": {},
   "source": [
    "In the above cells, I am using my bearer token that Twitter has given me, once they approved my use case for this project. The point of this is to authenticate to Twitter that I am verified and allowed to access this data at the 'Standard' level that I have been approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bd2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = 'https://api.twitter.com/2/tweets/search/recent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21494c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = urllib.parse.quote('(album (Friday OR Thursday OR Monday OR Tuesday OR Wednesday OR Saturday OR Sunday) (dropping OR release OR releasing OR (coming out)) lang:en -is:retweet)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae002671",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_fields = 'public_metrics,created_at,author_id,conversation_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7def4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = endpoint_url + '?query={}&max_results=100&tweet.fields={}&user.fields={}&expansions={}'.format(query, tweet_fields, 'username', 'author_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd5cf6",
   "metadata": {},
   "source": [
    "In the above four cells, I am creating my query. First you must start out with the endpoint URL that Twitter provides in their documentation. Next, I am creating the query and using a package called 'urllib' that will take the readable query that I have created and turn it into html text that Twitter will be able to understand and use in my request. I then finalize the URL that I am sending twitter by asking for specific tweet fields that I want this data to show -- I decided to add in conversation_id as a field, so I knew if people were talking about the same albums in the same thread, or if these are unique tweets being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0096dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1 = requests.request(\"GET\", url, headers = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb823b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_1_dict = json.loads(response_1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec325cb",
   "metadata": {},
   "source": [
    "#### Sending the request\n",
    "\n",
    "\n",
    "Above, this is the the actual request that I am sending to twitter, using the URL and header I have already defined. Then I am turning the data that Twitter sends back into a JSON that I can parse using Python. The raw data structure that Twitter sends, while you may be able to look at it and grab some things out, can't be utilized with these Python techniques, without it being turned into a JSON list or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0868fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = pd.DataFrame(response_1_dict['includes']['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4cdce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame(response_1_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a5c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['name'] = user_info['name']\n",
    "my_df['username'] = user_info['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a5124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53653d",
   "metadata": {},
   "source": [
    "#### Adding columns to a dataframe\n",
    "\n",
    "Above, I am adding both the username and name columns to what would be the default pandas dataframe of the json that Twitter has given me. In order to do this, I am making another dataframe with the 'includes' key so that I can get user data into a dataframe. Once I've done that (and created the default dataframe with 'data'), I then go ahead and add both 'name' and 'username' columns to the same dataframe. Throughout the rest of the code, I will repeat that process, and will refer back to this markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e34c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2 = url + '&next_token={}'.format(response_1_dict['meta']['next_token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2b59d",
   "metadata": {},
   "source": [
    "#### Pagination\n",
    "\n",
    "Here I am using a technique called pagination, to get more results from my request. Twitter only allows up to 100 results per query to show up, however it gives you access to more -- just buried under this process. Under the 'meta' key, it gives you a piece of data called 'next_token', and when you add this token on top of the URL you have built, it will go to the next 'page' of data, and show you those next 100 results. This process can be repeated and I will do it one more time, but you must clear the old 'next_token' in order for it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01772bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = requests.request(\"GET\", url_2, headers = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4700c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2_dict = json.loads(response_2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac0d5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info_2 = pd.DataFrame(response_2_dict['includes']['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "688ce4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df2 = pd.DataFrame(response_2_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6757ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df2['name'] = user_info_2['name']\n",
    "my_df2['username'] = user_info_2['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52da05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d641b",
   "metadata": {},
   "source": [
    "Above, I am repeating the processes I have already explained in sending the request and adding columns to the data frame. However, just creating new variables to signify that this is the second dataframe that I am trying to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb864d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_3 = url + '&next_token={}'.format(response_2_dict['meta']['next_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00612e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_3 = requests.request(\"GET\", url_3, headers = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d31a865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_3_dict = json.loads(response_3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61a41f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info_3 = pd.DataFrame(response_3_dict['includes']['users'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d148ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df3 = pd.DataFrame(response_3_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34c57be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df3['name'] = user_info_3['name']\n",
    "my_df3['username'] = user_info_3['username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0236831",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1f5d3",
   "metadata": {},
   "source": [
    "For the third time, I have repeated the process of creating a unique URL with pagination, sending the request, and then adding in the additional columns that I want to be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1b771d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_df = my_df.append(my_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d10842e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = int_df.append(my_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105292cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fin_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18255a6",
   "metadata": {},
   "source": [
    "In these above cells, I am appending the three dataframes I have created into a final dataframe named 'fin_df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "419944ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df.to_csv(\"Twitter_album_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2d4f1",
   "metadata": {},
   "source": [
    "Finally, I am exporting this final dataframe to a CSV file, to be viewed on Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d0b50",
   "metadata": {},
   "source": [
    "### Final Reflection and Assessment\n",
    "\n",
    "Looking at the data that I have gathered from Twitter, I think that it for the most part accomplishes what I set out to gather. Looking at the text from the tweets, it seems that these are all the types of tweets that I was looking for: tweets anticipating the release of a new album. Having the conversation ID would also be helpful in later steps of analysis, for example if I were to take this into a Pivot Table in excel, I could group these tweets by conversation ID to look at which tweets are generating the most in-thread responses.\n",
    "\n",
    "However, there may still be some limitations in this data. For one, it doesn't take into account the sentiment of these conversations that are happening: how people feel about these albums. Also, because it only runs once, and not on a continuous basis, the data is likely influenced by what day this query is run, which could bias the results.\n",
    "\n",
    "An alternative approach that I may want to take in the future, is to create a function for this process which would repeat itself everyday, so that the data gathered would remove bias in that sense. I would also likely want to create a function to simply repeat the steps of creating the URL, sending the request, and adding columns to the dataframe. This would save many lines of code, and would be easier to repeat this pagination process as much as I want.\n",
    "\n",
    "Editors note: I have cleared the output of the dataframes in the final submission, as to save space when reading through the code, because Github does not condense dataframes like Jupyter Notebook does."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
